{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the authentication flow. if you want to user json, please comment this\n",
    "ee.Authenticate()\n",
    "# Initialize the library\n",
    "# ee.Initialize(project='bukit30project')\n",
    "# ee.Initialize(project='treeo-1')\n",
    "ee.Initialize(project='ee-iwansetiawan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir:  C:/Users/q_bal/Documents/Github/GEE_notebook_Forestry/osi/arcpy\n",
      "config--->  {'module_path': 'C:/Users/q_bal/Documents/Github/GEE_notebook_Forestry/', 'I_satellite': 'Planet', 'pca_scaling': 1, 'tileScale': 1, 'AOI_path': 'G:/Shared drives/TREEO BD Supply/satellite_verification/Colombia/aoi_colombia.shp', 'OID_field_name': 'id', 'input_training': 'G:/Shared drives/TREEO BD Supply/satellite_verification/Colombia/trainingpoint_matarredonda.shp', 'algo_ml_selected': 'gbm', 'date_start_end': ['2024-8-1', '2024-8-31'], 'project_name': 'project_colombia', 'super_pixel_size': 3, 'region': 'america', 'pixel_number': 3, 'year_start_loss': 14, 'tree_cover_forest': 30, 'band_name_image': 'Class', 'cloud_cover_threshold': 40, 'crs_input': 'EPSG:4326', 'IsThermal': False, 'fcd_selected': 21, 'high_forest': 75, 'yrf_forest': 55, 'shrub_grass': 45, 'open_land': 30, 'ndwi_hi_sentinel': 0.05, 'ndwi_hi_landsat': 0.1, 'ndwi_hi_planet': -0.2}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# variable\n",
    "# Connect to the path of the module\n",
    "module_path = 'C:/Users/q_bal/Documents/Github/GEE_notebook_Forestry/'\n",
    "map_name_arcgis_pro = 'eligibility_check_prospective_tpps'\n",
    "create_training_gee = False\n",
    "\n",
    "# Add the module path to sys.path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "current_dir = os.path.join(module_path,\"osi/arcpy\")\n",
    "print('current_dir: ',current_dir)\n",
    "\n",
    "# Move to the parent directory of the current script\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Construct the absolute path to the JSON file in the 'input' folder\n",
    "json_path= os.path.join(parent_dir, '00_input', 'colombia_project.json')\n",
    "\n",
    "# Read and load the JSON data from the file\n",
    "with open(json_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "print('config---> ',config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import main library\n",
    "import ee\n",
    "import geemap\n",
    "import osi\n",
    "import pandas as pd\n",
    "\n",
    "from osi.utils.main import validate_aoi\n",
    "# convert the modules for image collection (cloudless masking, compositing, reducer etc)\n",
    "from osi.image_collection.main import ImageCollection\n",
    "from osi.spectral_indices.spectral_analysis import SpectralAnalysis\n",
    "from osi.spectral_indices.utils import normalization_100\n",
    "from osi.hansen.historical_loss import HansenHistorical\n",
    "from osi.classifying.assign_zone import AssignClassZone\n",
    "from osi.legends.utils import convert_to_legend_items\n",
    "from osi.legends.main import LegendsBuilder\n",
    "from osi.obia.main import OBIASegmentation\n",
    "from osi.ml.main import LandcoverML\n",
    "from osi.arcpy.main import ArcpyOps\n",
    "from osi.fcd.main_fcd import FCDCalc\n",
    "from osi.pca.pca_gee import PCA\n",
    "from osi.hansen.historical_loss import HansenHistorical\n",
    "from osi.classifying.assign_zone import AssignClassZone\n",
    "from osi.arcpy.utils import safe_get_data_source\n",
    "\n",
    "\n",
    "AOIt_shp_plot = geemap.shp_to_ee(config['AOI_path'])\n",
    "crs_input = config['crs_input']\n",
    "I_satellite = config['I_satellite']\n",
    "project_name = config['project_name']\n",
    "\n",
    "start_date = config['date_start_end'][0]\n",
    "end_date = config['date_start_end'][1]\n",
    "\n",
    "layer_name_image_mosaick = f'image_mosaick_result_ee_{project_name}'\n",
    "\n",
    "AOI = AOIt_shp_plot\n",
    "config['AOI'] = AOI\n",
    "\n",
    "ndwi_hi = 0.1\n",
    "if config['I_satellite'] == 'Landsat':\n",
    "    ndwi_hi = config['ndwi_hi_landsat']\n",
    "elif I_satellite == 'Sentinel':\n",
    "    ndwi_hi = config['ndwi_hi_sentinel']\n",
    "elif I_satellite == 'Planet':\n",
    "    ndwi_hi = config['ndwi_hi_planet']\n",
    "\n",
    "### Masking and overlay and area helper Make an image out of the AOI area attribute -> convert featurecollection into raster (image) for overlaying tools\n",
    "OID = config['OID_field_name']\n",
    "AOI_img = AOI.filter(ee.Filter.notNull([OID])).reduceToImage(\n",
    "    properties= [OID],\n",
    "    reducer= ee.Reducer.first()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD FIRST AOI TO ARCGIS MAP - option-1 in using existing shp to geemap\n",
    "# aoi_layer_name_in_map_arcgis = 'aoi_extent_delight'\n",
    "\n",
    "# USE THE PATH FOR TRAINING DATA - option-2 in using existing shp to geemap\n",
    "# path_shp_input_training = r'G:\\Shared drives\\TREEO BD Supply\\02. UGA\\01. TPPs\\03. Delight Ltd\\Due Diligence\\2024_07_24_Updated_Satellite_Assessment\\raw_data_shp_script\\00_input\\training_aoi_smaller.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will select the map for eligibility_check_prospective_tpps\n",
      "adding aoi_colombia\n",
      "path of the AOI: G:\\Shared drives\\TREEO BD Supply\\satellite_verification\\Colombia\\aoi_colombia.shp\n"
     ]
    }
   ],
   "source": [
    "# empower the capability of arcpy, native arcgis scripting from a class made\n",
    "# Open the currently active project, you can also refer to actual project path, comment here below, and activate after\n",
    "project_path = \"CURRENT\"\n",
    "# project_path = r'G:\\My Drive\\TreeO_WORKS\\GIS_data\\ArcGIS_Pro\\TREEO\\TREEO.aprx'\n",
    "arc_ops= ArcpyOps(project_path_arcgis = project_path, map_name_arcgis= map_name_arcgis_pro)\n",
    "map = arc_ops.map\n",
    "# layer_aoi = arc_ops.selecting_layer(name_layer = aoi_layer_name_in_map_arcgis)\n",
    "AOIt_shp = arc_ops.select_adding_layer(config['AOI_path'])['layer_path']\n",
    "AOI = geemap.shp_to_ee(AOIt_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.normpath(config['AOI_path']) not in arc_ops.list_source_layers_in_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding aoi_colombia\n",
      "path of the AOI: G:\\Shared drives\\TREEO BD Supply\\satellite_verification\\Colombia\\aoi_colombia.shp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>Shape</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(-73.38642615292376, 3.5280377964904033)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FID                                     Shape  id\n",
       "0    0  (-73.38642615292376, 3.5280377964904033)   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'id' found. Proceeding with operations masking based on AOI \n",
      "please continue\n"
     ]
    }
   ],
   "source": [
    "# lets print below the layer and convert into df if field names is exist\n",
    "# Get the layer's attribute table fields\n",
    "layer_aoi_arc = arc_ops.select_adding_layer(config['AOI_path'])['layer']\n",
    "fields = [field.name for field in arcpy.ListFields(layer_aoi_arc)]\n",
    "\n",
    "# Initialize an empty list to hold the data\n",
    "data = []\n",
    "\n",
    "# Use a SearchCursor to iterate over the rows in the attribute table\n",
    "with arcpy.da.SearchCursor(layer_aoi_arc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append(row)\n",
    "\n",
    "# Create a pandas DataFrame from the data\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "display(df)\n",
    "\n",
    "#for area id in shapefile that identified the data, and will converted into raster\n",
    "OID = config['OID_field_name']  #IMPORTANT TO CHECK OID based on the column ID\n",
    "if OID not in fields:\n",
    "    print(f'field_name of {OID} is not exist ERROR WILL HAPPEN!!!!')\n",
    "    raise ValueError(f\"Field '{OID}' not found in the fields: {fields}\")\n",
    "else:\n",
    "    print(f\"Field '{OID}' found. Proceeding with operations masking based on AOI \\nplease continue\")\n",
    "    # Proceed with further operations, like converting to raster, etc.\n",
    "    #############################################\n",
    "    ##################################################################################\n",
    "    ### Masking and overlay and area helper Make an image out of the AOI area attribute -> convert featurecollection into raster (image) for overlaying tools\n",
    "    AOI_img = AOI.filter(ee.Filter.notNull([OID])).reduceToImage(\n",
    "        properties= [OID],\n",
    "        reducer= ee.Reducer.first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate the AOI input: \n",
      "trying to list the featurecollection\n",
      "now for loop to feature size range\n",
      "All features have a valid \"id\" column with integer values.\n",
      "adding trainingpoint_matarredonda\n",
      "path of the AOI: G:\\Shared drives\\TREEO BD Supply\\satellite_verification\\Colombia\\trainingpoint_matarredonda.shp\n",
      "validate input training points\n",
      "before_validation:  (80, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>Shape</th>\n",
       "      <th>id</th>\n",
       "      <th>code_lu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(-73.38739351932064, 3.5299446222190913)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(-73.36911663366908, 3.5211088775804913)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(-73.38707029590635, 3.5223413839527273)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(-73.38855229234295, 3.524712657314634)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(-73.37309987600794, 3.537835365579796)</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>(-73.39876455724504, 3.5155823204940946)</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>(-73.4039870480688, 3.5232642042123166)</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>(-73.38189490269305, 3.525276557555658)</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>(-73.39087969551139, 3.5342647709142083)</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>(-73.37636930096099, 3.5223890846812376)</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FID                                     Shape  id  code_lu\n",
       "0     0  (-73.38739351932064, 3.5299446222190913)   0        4\n",
       "1     1  (-73.36911663366908, 3.5211088775804913)   1        1\n",
       "2     2  (-73.38707029590635, 3.5223413839527273)   2        4\n",
       "3     3   (-73.38855229234295, 3.524712657314634)   3        1\n",
       "4     4   (-73.37309987600794, 3.537835365579796)   4        4\n",
       "..  ...                                       ...  ..      ...\n",
       "75   75  (-73.39876455724504, 3.5155823204940946)  75        1\n",
       "76   76   (-73.4039870480688, 3.5232642042123166)  76        1\n",
       "77   77   (-73.38189490269305, 3.525276557555658)  77        1\n",
       "78   78  (-73.39087969551139, 3.5342647709142083)  78        1\n",
       "79   79  (-73.37636930096099, 3.5223890846812376)  79        1\n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after validation:  (80, 4)\n"
     ]
    }
   ],
   "source": [
    "# smarter way to check\n",
    "print('validate the AOI input: ')\n",
    "validate_aoi(AOI, ee, config['OID_field_name'])\n",
    "\n",
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "input_training = arc_ops.select_adding_layer(config['input_training'])['layer_path']\n",
    "input_training_ee = geemap.shp_to_ee(config['input_training'])\n",
    "\n",
    "print('validate input training points')\n",
    "fields_input_training = [field.name for field in arcpy.ListFields(input_training)]\n",
    "\n",
    "# Initialize an empty list to hold the data\n",
    "data_input_training = []\n",
    "\n",
    "# Use a SearchCursor to iterate over the rows in the attribute table\n",
    "with arcpy.da.SearchCursor(input_training, fields_input_training) as cursor:\n",
    "    for row in cursor:\n",
    "        data_input_training.append(row)\n",
    "\n",
    "# Create a pandas DataFrame from the data\n",
    "df = pd.DataFrame(data_input_training, columns=fields_input_training)\n",
    "print('before_validation: ',df.shape)\n",
    "# Function to check if a value is an integer\n",
    "def is_integer(value):\n",
    "    return isinstance(value, int)\n",
    "\n",
    "# Filter out non-integer values in the 'code_lu' column\n",
    "df['code_lu'] = df['code_lu'].apply(lambda x: x if is_integer(x) else None)\n",
    "display(df)\n",
    "print('after validation: ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [f.name for f in map.listLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [f.name for f in arc_ops.list_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now starting to do analysis\n",
    "# initiate instance class for the image collection and later mosaicking\n",
    "classInputCollection = ImageCollection(I_satellite=I_satellite,\n",
    "                                       AOI=AOI, \n",
    "                                       date_start_end=config['date_start_end'], \n",
    "                                       cloud_cover_threshold = config['cloud_cover_threshold'],\n",
    "                                       region=config['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting Planet images\n"
     ]
    }
   ],
   "source": [
    "# run the method from image collection loaded, cloudless compositing until to image_mosaick\n",
    "image_mosaick = classInputCollection.image_mosaick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/3272a7bd46174728434a38f8a780aaf4-7db0e915ad4839d7bbbea36090ea6e31/tiles/{z}/{x}/{y}\n"
     ]
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "vis_params_image_mosaick = {\"bands\":[\"red\",\"green\",\"blue\"],\"min\":0,\"max\":0.1,\"gamma\":1}\n",
    "layer_name_image_mosaick = f'image_mosaick_result_ee_{config[\"project_name\"]}'\n",
    "# image_mosaick_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(image_mosaick, vis_params_image_mosaick,\n",
    "#                                                    layer_name_image_mosaick)\n",
    "image_mosaick_arcgis_layer = None\n",
    "\n",
    "if config['I_satellite'] == 'Planet':\n",
    "    # true color {\"bands\":[\"red\",\"green\",\"blue\"],\"min\":0,\"max\":0.6,\"gamma\":1.5}\n",
    "    # nir veg color {\"bands\":[\"red\",\"nir\",\"blue\"],\"min\":0,\"max\":0.6,\"gamma\":1.5 }\n",
    "    image_mosaick_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(image_mosaick,{\"bands\":[\"red\",\"green\",\"blue\"],\"min\":0,\"max\":0.6,\"gamma\":1.5}, f'{I_satellite} mosaicked - {start_date}-{end_date} VegColor')\n",
    "else:\n",
    "    image_mosaick_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(image_mosaick,{'bands': ['swir2', 'nir', 'red'], 'min': 0, 'max': 0.6, 'gamma': 1.5 }, f'{I_satellite} mosaicked - {start_date}-{end_date}')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting Planet images\n",
      "selecting Planet images\n",
      "processing AVI\n",
      "processing BSI\n",
      "processing SI\n",
      "Normalizing to 100 AVI\n",
      "Normalizing to 100 AVI\n",
      "Normalizing to 100 BSI\n",
      "Normalizing to 100 SI\n",
      "Combining AVI AND BSI\n",
      "no thermal band, choosing Planet images\n",
      "Processing means center of AVI_BSI please wait\n",
      "Now we proceed to the PCA of Vegetation density\n",
      "Success get the PCA normalized of VD => SVI\n",
      "Now calculating the FCD from SVI and SSI - selecting band svi1 svi2 ssi1 and ssi2\n",
      "finish processing PCA, the result: FCD1_1 and FCD2_1 please continue\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/3f416e3c2c63a65d56f008cef7c292a3-9e6e48a6ebfe3aa82a2d27d91274a771/tiles/{z}/{x}/{y}\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/daa1e82e259e78438f426d14d3265911-92c344f904bc651670b1d10665ffd189/tiles/{z}/{x}/{y}\n",
      "finish processing PCA please continue\n"
     ]
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "classImageSpectral = SpectralAnalysis(image_mosaick,config)\n",
    "class_FCD_run = FCDCalc(config).fcd_calc()\n",
    "FCD1_1 = class_FCD_run['FCD1_1']\n",
    "FCD2_1 = class_FCD_run['FCD2_1']\n",
    "\n",
    "FCD1_1_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(FCD1_1, {'min':0 ,'max':80, 'palette':['ff4c16', 'ffd96c', '39a71d']},\n",
    "                                                   f'FCD1_1_{project_name}')\n",
    "\n",
    "FCD2_1_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(FCD2_1, {'min':0 ,'max':80, 'palette':['ff4c16', 'ffd96c', '39a71d']},\n",
    "                                                   f'FCD2_1_{project_name}')\n",
    "\n",
    "print('finish processing PCA please continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is historical data to overlay later with current baseline (landcover)\n",
    "hansen_class = HansenHistorical(config)\n",
    "run_hansen = hansen_class.initiate_tcl()\n",
    "LastImageLandsat, treeLossYear, minLoss, ForestArea2000Hansen, gfc =  \\\n",
    "                                 run_hansen['LastImageLandsat'], \\\n",
    "                                 run_hansen['treeLossYear'], \\\n",
    "                                 run_hansen['minLoss'], \\\n",
    "                                 run_hansen['ForestArea2000Hansen'], \\\n",
    "                                 run_hansen['gfc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the map of Forest, FCD >= 55% and mask only if not water in area (hansen) and NDWI\n",
      "Adding the map of Shrubland, FCD <  55% and FCD >= 45%\n",
      "Adding the map of Grassland or Openland, FCD  < 45%\n",
      "Processing - the zoning classification\n",
      "finish processing, merging all the zone into one image\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/da05071f286f3d4b5dd49415106e6382-13e84c5ef1b30038c6417df615d9fb89/tiles/{z}/{x}/{y}\n"
     ]
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "config['AOI_img'] = AOI_img\n",
    "\n",
    "class_assigning_fcd =  AssignClassZone(config, FCD1_1=FCD1_1, FCD2_1=FCD2_1)\n",
    "list_images_classified = class_assigning_fcd.assigning_fcd_class(gfc, minLoss)\n",
    "\n",
    "fcd_classified_zone = list_images_classified['all_zone']\n",
    "\n",
    "vis_params_fcd_classified = class_assigning_fcd.vis_param_merged\n",
    "# Convert the dictionary to the LEGEND_ITEMS format\n",
    "legend_items = convert_to_legend_items(class_assigning_fcd.legend_class)\n",
    "\n",
    "fcd_classified_zone_arcgis_layer = arc_ops.adding_ee_to_arcgisPro(fcd_classified_zone, vis_params_fcd_classified,\n",
    "                                                   f'FCD_classified_zone_{project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDING DIRECTLY SPECTRAL INDICES\n",
    "# classImageSpectral\n",
    "pca_scale = classImageSpectral.pca_scale #pca_scale is spatial resolution. eg planet: 5\n",
    "ndwi_image = classImageSpectral.NDWI_func()\n",
    "msavi2_image = classImageSpectral.MSAVI2_func()\n",
    "mtvi2_image = classImageSpectral.MTVI2_func()\n",
    "ndvi_image = classImageSpectral.NDVI_func()\n",
    "vari_image = classImageSpectral.VARI_func()\n",
    "\n",
    "image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari = (\n",
    "    image_mosaick\n",
    "    .addBands(ndwi_image)\n",
    "    .addBands(msavi2_image)\n",
    "    .addBands(mtvi2_image)\n",
    "    .addBands(ndvi_image)\n",
    "    .addBands(vari_image)\n",
    ")\n",
    "\n",
    "red_norm = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select(['red']), pca_scale=pca_scale, AOI=AOI)\n",
    "green_norm = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select(['green']), pca_scale=pca_scale, AOI=AOI)\n",
    "blue_norm = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select(['blue']), pca_scale=pca_scale, AOI=AOI)\n",
    "nir_norm = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select(['nir']), pca_scale=pca_scale, AOI=AOI)\n",
    "\n",
    "image_norm = red_norm.addBands(green_norm).addBands(blue_norm).addBands(nir_norm)\n",
    "\n",
    "image_norm_ndvi = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('NDVI'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_ndwi = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('ndwi'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_msavi2 = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('msavi2'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_mtvi2 = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('MTVI2'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_vari = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('VARI'), pca_scale=pca_scale, AOI=AOI)\n",
    "\n",
    "# red_norm.bandNames().getInfo()\n",
    "image_norm_with_spectral_indices = image_norm.addBands(image_norm_ndvi).addBands(image_norm_ndwi).addBands(image_norm_msavi2).addBands(image_norm_mtvi2).addBands(image_norm_vari)\n",
    "image_norm_with_spectral_indices_FCD = image_norm_with_spectral_indices.addBands(FCD2_1.select('FCD').rename('FCD2_1')).addBands(FCD1_1.select('FCD').rename('FCD1_1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snic list bands: ['red_mean', 'green_mean', 'blue_mean', 'nir_mean', 'ndwi_mean', 'msavi2_mean', 'MTVI2_mean', 'NDVI_mean', 'VARI_mean', 'FCD1_1_mean', 'FCD2_1_mean', 'area', 'clusters_min', 'width', 'height']\n"
     ]
    }
   ],
   "source": [
    "obia = OBIASegmentation(config=config, image=image_norm_with_spectral_indices_FCD, pca_scale=pca_scale) #pca_scale basically is spatial resolution e.g planet: 5\n",
    "clusters = obia.SNIC_cluster()['clusters']\n",
    "object_properties_image = obia.summarize_cluster(is_include_std = False)\n",
    "\n",
    "lc = LandcoverML(config=config,\n",
    "                 input_image = image_norm_with_spectral_indices_FCD,\n",
    "                cluster_properties=object_properties_image,\n",
    "                pca_scale = pca_scale)\n",
    "\n",
    "classifier = lc.run_classifier()\n",
    "\n",
    "legend_lc = lc.lc_legend_param()\n",
    "vis_param_lc = legend_lc['vis_param_lc']\n",
    "\n",
    "legend_lc = legend_lc['legend_class']\n",
    "# Convert the dictionary to the LEGEND_ITEMS format\n",
    "legend_items_lc = convert_to_legend_items(legend_lc)\n",
    "\n",
    "training_points = classifier['training_points']\n",
    "validation_points = classifier['validation_points']\n",
    "\n",
    "rf = classifier['classified_image_rf']\n",
    "svm = classifier['classified_image_svm']\n",
    "gbm = classifier['classified_image_gbm']\n",
    "cart = classifier['classified_image_cart']\n",
    "\n",
    "# fcd lc, 5 classes only, just nice to know\n",
    "fcd_lc = list_images_classified['fcd_class_lc_image']\n",
    "fcd_lc_vs = list_images_classified['vis_param_segment_lc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mosaick_all_bands = image_mosaick.addBands([FCD2_1.select('FCD').rename('FCD2_1'), FCD1_1.select('FCD').rename('FCD1_1')])\n",
    "\n",
    "## ADDING DIRECTLY SPECTRAL INDICES\n",
    "# classImageSpectral\n",
    "pca_scale = classImageSpectral.pca_scale\n",
    "ndwi_image = classImageSpectral.NDWI_func()\n",
    "msavi2_image = classImageSpectral.MSAVI2_func()\n",
    "mtvi2_image = classImageSpectral.MTVI2_func()\n",
    "ndvi_image = classImageSpectral.NDVI_func()\n",
    "vari_image = classImageSpectral.VARI_func()\n",
    "\n",
    "image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari = (\n",
    "    image_mosaick_all_bands\n",
    "    .addBands(ndwi_image)\n",
    "    .addBands(msavi2_image)\n",
    "    .addBands(mtvi2_image)\n",
    "    .addBands(ndvi_image)\n",
    "    .addBands(vari_image)\n",
    ")\n",
    "\n",
    "red_norm = normalization_100(image_mosaick.select(['red']), pca_scale=pca_scale, AOI=AOI)\n",
    "green_norm = normalization_100(image_mosaick.select(['green']), pca_scale=pca_scale, AOI=AOI)\n",
    "blue_norm = normalization_100(image_mosaick.select(['blue']), pca_scale=pca_scale, AOI=AOI)\n",
    "nir_norm = normalization_100(image_mosaick.select(['nir']), pca_scale=pca_scale, AOI=AOI)\n",
    "\n",
    "image_norm = red_norm.addBands(green_norm).addBands(blue_norm).addBands(nir_norm)\n",
    "\n",
    "image_norm_ndvi = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('NDVI'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_ndwi = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('ndwi'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_msavi2 = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('msavi2'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_mtvi2 = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('MTVI2'), pca_scale=pca_scale, AOI=AOI)\n",
    "image_norm_vari = normalization_100(image_mosaick_ndvi_ndwi_msavi2_mtvi2_vari.select('VARI'), pca_scale=pca_scale, AOI=AOI)\n",
    "\n",
    "# red_norm.bandNames().getInfo()\n",
    "image_norm_with_spectral_indices = image_norm.addBands(image_norm_ndvi).addBands(image_norm_ndwi).addBands(image_norm_msavi2).addBands(image_norm_mtvi2).addBands(image_norm_vari)\n",
    "image_norm_with_spectral_indices_FCD = image_norm_with_spectral_indices.addBands(FCD2_1.select('FCD').rename('FCD2_1')).addBands(FCD1_1.select('FCD').rename('FCD1_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snic list bands: ['red_mean', 'green_mean', 'blue_mean', 'nir_mean', 'ndwi_mean', 'msavi2_mean', 'MTVI2_mean', 'NDVI_mean', 'VARI_mean', 'FCD1_1_mean', 'FCD2_1_mean', 'area', 'clusters_min', 'width', 'height']\n"
     ]
    }
   ],
   "source": [
    "obia = OBIASegmentation(config=config, image=image_norm_with_spectral_indices_FCD, pca_scale=pca_scale) #pca_scale basically is spatial resolution e.g planet: 5\n",
    "clusters = obia.SNIC_cluster()['clusters']\n",
    "object_properties_image = obia.summarize_cluster(is_include_std = False)\n",
    "# make sure has all the same type of data in all bands, for exporting purpose\n",
    "object_properties_image = object_properties_image.clip(AOI).toFloat()\n",
    "\n",
    "lc = LandcoverML(config=config,\n",
    "                 input_image = image_norm_with_spectral_indices_FCD,\n",
    "                cluster_properties=object_properties_image,\n",
    "                 num_class=5, # make sure this one is align with total type landcover stratification, for a sample creation\n",
    "                pca_scale = pca_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/53ed17e2aa05831a59f48945d4376446-5513bae5d3d3372a6eb7e2803405c68e/tiles/{z}/{x}/{y}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<arcpy._mp.Layer object at 0x000002C441416690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_ops.adding_ee_to_arcgisPro(clusters.randomVisualizer(),{},'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use the existing training_points or labelled\n",
      "location of the training sample is in G:/Shared drives/TREEO BD Supply/satellite_verification/Colombia/trainingpoint_matarredonda.shp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create stratified random sampling based on K-means classes\n",
    "# create_training_gee\n",
    "if create_training_gee:\n",
    "    print('yes')\n",
    "    \n",
    "    ########### SAMPLE NUMBER CREATION BASED ON https://docs.google.com/spreadsheets/d/1J8MEi4IDn6faok6UUn9L64T61yWk0D4q/edit?gid=1919918133#gid=1919918133\n",
    "#     # example\n",
    "#     strata_area_based_kmeans = {\n",
    "#     'Forest': 100,\n",
    "#     'Shrub': 100,\n",
    "#     'Grass': 100,\n",
    "#     'Crop': 100,\n",
    "#     'Water': 100\n",
    "#     }\n",
    "    \n",
    "    # try with K-means input\n",
    "    random_samples_creation = lc.stratified_random_creation()\n",
    "    df_sample_n = random_samples_creation['df_sample_n']\n",
    "    stratified_training = random_samples_creation['stratified_training']\n",
    "       \n",
    "    # Export the samples to a CSV file in Google Drive\n",
    "    export_stratified_point = ee.batch.Export.table.toDrive(\n",
    "        collection=stratified_training,\n",
    "        description=f'Stratified_Random_Samples_{project_name}',\n",
    "        folder=f'lu_input_{project_name}',\n",
    "        fileFormat='SHP'\n",
    "    )\n",
    "\n",
    "    # Start the export task\n",
    "    export_stratified_point.start()\n",
    "\n",
    "    # Monitor the task status\n",
    "    import time\n",
    "    while export_stratified_point.active():\n",
    "        print('Export task status:', export_stratified_point.status())\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(f'Export task completed: Stratified_Random_Samples_{project_name}')\n",
    "    \n",
    "    # Open in gdrive in your computer\n",
    "    location_stratified_exported = fr'G:\\My Drive\\lu_input_{project_name}\\Stratified_Random_Samples_{project_name}.shp'\n",
    "    print(location_stratified_exported) #make sure the location is correct (G: is connected by gdrive desktop!)\n",
    "    \n",
    "    # arcgis layer object\n",
    "    training_points = map.addDataFromPath(location_stratified_exported)\n",
    "    # training_points.name = 'change the name here'\n",
    "    \n",
    "    path_shp_input_training = training_points.dataSource\n",
    "    \n",
    "else:\n",
    "    print('we will use the existing training_points or labelled')\n",
    "    print(f'location of the training sample is in {config[\"input_training\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/f26c67dd0b4aa247de0f19e454ee1270-43efef6d33d45a5ef5acda13eb97269f/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: fcd-method_lc_result\n",
      "layer re-added: fcd-method_lc_result\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/ffd03bcfda29f7922547c907e6e3f217-d1f097db0dcd1786be64b45156b4cf9b/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Random_forest_lc_result\n",
      "layer re-added: Random_forest_lc_result\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/0f1651f422250445abf9df0a3373f027-c45127f5a795dce3242c4aba46523819/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: SVM_lc_result\n",
      "layer re-added: SVM_lc_result\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/6a7a542b9f9149d2752a255b66b5aedd-55bc113129e2d6503c03c54511785cd5/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: GBM_lc_result\n",
      "layer re-added: GBM_lc_result\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/9133957cfbf7ce6dc577592991906b5d-7e72dcd7de57fa71924b34a6d2a8690b/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: CART_lc_result\n",
      "layer re-added: CART_lc_result\n"
     ]
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "# starting to do ML analysis\n",
    "classifier = lc.run_classifier()\n",
    "\n",
    "legend_lc = lc.lc_legend_param()\n",
    "vis_param_lc = legend_lc['vis_param_lc']\n",
    "\n",
    "training_points = classifier['training_points']\n",
    "validation_points = classifier['validation_points']\n",
    "\n",
    "rf = classifier['classified_image_rf']\n",
    "svm = classifier['classified_image_svm']\n",
    "gbm = classifier['classified_image_gbm']\n",
    "cart = classifier['classified_image_cart']\n",
    "\n",
    "# fcd lc, 5 classes only, just nice to know\n",
    "fcd_lc = list_images_classified['fcd_class_lc_image']\n",
    "fcd_lc_vs = list_images_classified['vis_param_segment_lc']\n",
    "fcd_lc_arc = arc_ops.adding_ee_to_arcgisPro(fcd_lc,fcd_lc_vs, 'fcd-method_lc_result')\n",
    "rf_arc = arc_ops.adding_ee_to_arcgisPro(rf,vis_param_lc,'Random_forest_lc_result')\n",
    "svm_arc = arc_ops.adding_ee_to_arcgisPro(svm,vis_param_lc,'SVM_lc_result')\n",
    "gbm_arc = arc_ops.adding_ee_to_arcgisPro(gbm,vis_param_lc,'GBM_lc_result')\n",
    "cart_arc = arc_ops.adding_ee_to_arcgisPro(cart,vis_param_lc,'CART_lc_result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "Algorithm of ML used: fcd\n",
      "Confusion Matrix:\n",
      "[[0, 0, 0, 0, 0], [0, 6, 2, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 3, 2, 1]]\n",
      "Overall Accuracy: 0.5\n",
      "Producer's Accuracy: [[0], [0.75], [0], [0], [0.16666666666666666]]\n",
      "User's Accuracy: [[0, 1, 0, 0, 1]]\n",
      "Kappa: 0.3098591549295775\n",
      "-------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------------\n",
      "Algorithm of ML used: rf\n",
      "Confusion Matrix:\n",
      "[[0, 0, 0, 0, 0], [0, 8, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 6]]\n",
      "Overall Accuracy: 1\n",
      "Producer's Accuracy: [[0], [1], [0], [0], [1]]\n",
      "User's Accuracy: [[0, 1, 0, 0, 1]]\n",
      "Kappa: 1\n",
      "-------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------------\n",
      "Algorithm of ML used: svm\n",
      "Confusion Matrix:\n",
      "[[0, 0, 0, 0, 0], [0, 8, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 6]]\n",
      "Overall Accuracy: 1\n",
      "Producer's Accuracy: [[0], [1], [0], [0], [1]]\n",
      "User's Accuracy: [[0, 1, 0, 0, 1]]\n",
      "Kappa: 1\n",
      "-------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------------\n",
      "Algorithm of ML used: gbm\n",
      "Confusion Matrix:\n",
      "[[0, 0, 0, 0, 0], [0, 8, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 6]]\n",
      "Overall Accuracy: 1\n",
      "Producer's Accuracy: [[0], [1], [0], [0], [1]]\n",
      "User's Accuracy: [[0, 1, 0, 0, 1]]\n",
      "Kappa: 1\n",
      "-------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------------\n",
      "Algorithm of ML used: cart\n",
      "Confusion Matrix:\n",
      "[[0, 0, 0, 0, 0], [0, 8, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 6]]\n",
      "Overall Accuracy: 1\n",
      "Producer's Accuracy: [[0], [1], [0], [0], [1]]\n",
      "User's Accuracy: [[0, 1, 0, 0, 1]]\n",
      "Kappa: 1\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lc.matrix_confusion(fcd_lc,validation_points,'fcd')\n",
    "lc.matrix_confusion(rf, validation_points, 'rf')\n",
    "lc.matrix_confusion(svm, validation_points, 'svm')\n",
    "lc.matrix_confusion(gbm, validation_points, 'gbm')\n",
    "lc.matrix_confusion(cart, validation_points, 'cart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo_ml_selected:  gbm\n"
     ]
    }
   ],
   "source": [
    "algo_ml_selected = 'rf'\n",
    "selected_image_lc = rf\n",
    "if config['algo_ml_selected'] == 'rf':\n",
    "    algo_ml_selected = 'rf'\n",
    "    selected_image_lc = rf\n",
    "elif config['algo_ml_selected'] == 'svm':\n",
    "    algo_ml_selected = 'svm'\n",
    "    selected_image_lc = svm\n",
    "elif config['algo_ml_selected'] == 'gbm':\n",
    "    algo_ml_selected = 'gbm'\n",
    "    selected_image_lc = gbm\n",
    "elif config['algo_ml_selected'] == 'cart':\n",
    "    algo_ml_selected = 'cart'\n",
    "    selected_image_lc = cart\n",
    "print('algo_ml_selected: ',algo_ml_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/d9f9c64dde448b2d4369c4e6d3547e51-eaf5ad8cfae69da034a5de25df73ecdf/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Final_zone_ML_gbm_Hansen\n",
      "layer re-added: Final_zone_ML_gbm_Hansen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<arcpy._mp.Layer object at 0x000002835FC08390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "\n",
    "# re-overlay the data for zoning from the selected method if they give the best metric, and when we check visually the land cover map make sense, also FCD approach is already there\n",
    "image_for_zone = selected_image_lc\n",
    "\n",
    "# comment this first, just check the LC above, then run the overlay zoning classification after\n",
    "HighForestDense = list_images_classified['HighForestDense']\n",
    "\n",
    "final_zone = class_assigning_fcd.assign_zone_ml(image_for_zone, minLoss,AOI_img, HighForestDense)\n",
    "arc_ops.adding_ee_to_arcgisPro(final_zone, vis_params_fcd_classified, f'Final_zone_ML_{algo_ml_selected}_Hansen')  # the naming probably will need to change, for some concistencies only so that you understand again later to read the codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/a93c6c8ef5e17274ffcc82bd7654d4b4-6805ba9321c05e8426326f5bcca61e6f/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Slope Classes\n",
      "layer re-added: Slope Classes\n",
      "finished adding slope\n",
      "Unique values: [16034, 16054]\n",
      "Value-Color Map: {16034: '#ebde16', 16054: '#d26f71'}\n",
      "{16054: 'Cambisols', 16034: 'Ferralsols'}\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/c096e6a77452dc6934f3e1dc5915be72-a0826b0116aceef8ea0c29eb2c3b310a/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: FAO_soil\n",
      "layer re-added: FAO_soil\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/06030609fa51968d40cd8617f302ca8b-e73fb0d0513592c064b330b3fcc1823f/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Training data location\n",
      "layer re-added: Training data location\n",
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/291f9dfec64e72ea0f335e08634dad69-88e36392b293db728be5d014cc742547/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Validation data location\n",
      "layer re-added: Validation data location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<arcpy._mp.Layer object at 0x000002C4401B6610>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "\n",
    "#additional data\n",
    "# Load DEM data (replace 'dataset' with your actual DEM dataset)\n",
    "DEM = ee.Image('USGS/SRTMGL1_003').clip(AOI)\n",
    "\n",
    "# Calculate slope in degrees\n",
    "slope = ee.Terrain.slope(DEM)\n",
    "\n",
    "# Convert slope to percentage\n",
    "slopePercentage = slope.expression('tan(b*0.01745) * 100', {'b': slope})\n",
    "\n",
    "# Define slope classification thresholds\n",
    "thresholds = [8, 15, 25, 40]  # Adjust these thresholds as needed\n",
    "\n",
    "# Classify slope into categories using conditional statements\n",
    "slopeClasses = slopePercentage \\\n",
    "    .lte(thresholds[0]).multiply(1) \\\n",
    "    .add(slopePercentage.gt(thresholds[0]).And(slopePercentage.lte(thresholds[1])).multiply(2)) \\\n",
    "    .add(slopePercentage.gt(thresholds[1]).And(slopePercentage.lte(thresholds[2])).multiply(3)) \\\n",
    "    .add(slopePercentage.gt(thresholds[2]).And(slopePercentage.lte(thresholds[3])).multiply(4)) \\\n",
    "    .add(slopePercentage.gt(thresholds[3]).multiply(5))\n",
    "\n",
    "# Display the classified slope image\n",
    "palette = ['lightgreen', 'yellow', 'orange', 'pink', 'red']  # Change green to lightgreen\n",
    "vis_params = {'min': 1, 'max': 5, 'palette': palette}\n",
    "# Map.addLayer(slopeClasses, vis_params, 'Slope Classes')\n",
    "arc_ops.adding_ee_to_arcgisPro(slopeClasses, vis_params, 'Slope Classes')  \n",
    "print('finished adding slope')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "## SOIL Overlay FAO\n",
    "FAO_soil = ee.Image(\"users/muhammadiqbaltreeo/HWSD2_FAO\").clip(AOI)\n",
    "# Get the unique values from the image\n",
    "unique_values = FAO_soil.reduceRegion(\n",
    "    reducer=ee.Reducer.frequencyHistogram(),\n",
    "    geometry=AOI.geometry(),\n",
    "    scale=30\n",
    ").getInfo()\n",
    "\n",
    "unique_values = list(unique_values['b1'].keys())\n",
    "unique_values = [int(f) for f in unique_values]\n",
    "print('Unique values:', unique_values)\n",
    "\n",
    "import random\n",
    "\n",
    "# Generate random colors for each unique value\n",
    "def get_random_color():\n",
    "    return \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "\n",
    "value_color_map = {value: get_random_color() for value in unique_values}\n",
    "print('Value-Color Map:', value_color_map)\n",
    "\n",
    "# Create a color palette based on the unique values and their corresponding colors\n",
    "palette = [value_color_map[value] for value in unique_values]\n",
    "\n",
    "# Create a visualization dictionary\n",
    "visualization = {\n",
    "    'min': min(unique_values),\n",
    "    'max': max(unique_values),\n",
    "    'palette': palette\n",
    "}\n",
    "\n",
    "smu_table = ee.FeatureCollection(\"users/muhammadiqbaltreeo/HWSD2_SMU\")\n",
    "\n",
    "# Get the data as a Python dictionary\n",
    "filtered_smu_table = smu_table.filter(ee.Filter.inList('HWSD2_SMU_ID', unique_values))\n",
    "\n",
    "# Get the filtered data as a Python dictionary\n",
    "filtered_smu_data = filtered_smu_table.getInfo()\n",
    "\n",
    "# Extract the features from the dictionary\n",
    "features = filtered_smu_data['features']\n",
    "\n",
    "# Extract properties from each feature\n",
    "properties_list = [feature['properties'] for feature in features]\n",
    "\n",
    "# Convert the list of properties to a pandas DataFrame\n",
    "df_snum = pd.DataFrame(properties_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(df_snum)\n",
    "\n",
    "# Extract the 'name' and 'snum' columns and convert them to a dictionary\n",
    "name_snum_dict = df_snum.set_index('HWSD2_SMU_ID')['name'].to_dict()\n",
    "print(name_snum_dict)\n",
    "\n",
    "# Update the legend dictionary to use the format \"snum: name_soil\"\n",
    "legend_dict = {f\"{snum}: {name_snum_dict[snum]}\": value_color_map[snum] for snum in unique_values}\n",
    "\n",
    "# Map.add_legend(title=\"Soil Type (FAO) Legend\", legend_dict=legend_dict)\n",
    "\n",
    "# Map.addLayer(FAO_soil.clip(AOI),visualization,'FAO_soil')\n",
    "arc_ops.adding_ee_to_arcgisPro(FAO_soil.clip(AOI),visualization,'FAO_soil')\n",
    "\n",
    "# Map.addLayer(AOIsmaller.style(**style), {}, 'AOI Smaller')\n",
    "# Map.addLayer(training_points, {'color': 'yellow'},\n",
    "#     'Training data location')\n",
    "arc_ops.adding_ee_to_arcgisPro(training_points, {'color': 'yellow'},\n",
    "    'Training data location')\n",
    "\n",
    "# Map.addLayer(validation_points, {'color': 'red'},\n",
    "#     'Validation data location')\n",
    "\n",
    "arc_ops.adding_ee_to_arcgisPro(validation_points, {'color': 'red'},\n",
    "    'Validation data location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend lc\n",
    "legend_class_lc = LegendsBuilder(legend_items=legend_items_lc)\n",
    "legend_class_lc.create_legend('landcover')\n",
    "\n",
    "# legend zone\n",
    "legend_class_zone = LegendsBuilder(legend_items=legend_items)\n",
    "legend_class_zone.create_legend('final-zone')\n",
    "\n",
    "# custom color spectrum - override example\n",
    "legend_class_lc.create_colorbar('Forest Canopy Density',{'min': 0, 'max': 100, 'palette': ['#ff4c16', '#ffd96c', '#39a71d']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some break of cell just to ensure if you want to export or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some break of cell just to ensure if you want to export or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some break of cell just to ensure if you want to export or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some break of cell just to ensure if you want to export or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export task status: {'state': 'READY', 'description': 'training_points_project_colombia', 'priority': 100, 'creation_timestamp_ms': 1725995650111, 'update_timestamp_ms': 1725995650111, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_FEATURES', 'id': 'NW57PGGX4XXSFFNYSC5W3HEA', 'name': 'projects/ee-iwansetiawan/operations/NW57PGGX4XXSFFNYSC5W3HEA'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'training_points_project_colombia', 'priority': 100, 'creation_timestamp_ms': 1725995650111, 'update_timestamp_ms': 1725995660191, 'start_timestamp_ms': 1725995657020, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'id': 'NW57PGGX4XXSFFNYSC5W3HEA', 'name': 'projects/ee-iwansetiawan/operations/NW57PGGX4XXSFFNYSC5W3HEA'}\n",
      "Export task completed: training_points_project_colombia\n",
      "location in gdrive (please add location drive letter) --> \\My Drive\\result_lu_class_zone_project_colombia\\training_points_project_colombia.shp\n",
      "Export task status: {'state': 'READY', 'description': 'validation_points_project_colombia', 'priority': 100, 'creation_timestamp_ms': 1725995672933, 'update_timestamp_ms': 1725995672933, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_FEATURES', 'id': 'R2UK464HBFM7UW5N3PAZL2JI', 'name': 'projects/ee-iwansetiawan/operations/R2UK464HBFM7UW5N3PAZL2JI'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'validation_points_project_colombia', 'priority': 100, 'creation_timestamp_ms': 1725995672933, 'update_timestamp_ms': 1725995684308, 'start_timestamp_ms': 1725995680614, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'id': 'R2UK464HBFM7UW5N3PAZL2JI', 'name': 'projects/ee-iwansetiawan/operations/R2UK464HBFM7UW5N3PAZL2JI'}\n",
      "Export task completed: validation_points_project_colombia\n",
      "location in gdrive (please add location drive letter) --> \\My Drive\\result_lu_class_zone_project_colombia\\validation_points_project_colombia.shp\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exporting_to_ee' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "In  \u001b[0;34m[39]\u001b[0m:\nLine \u001b[0;34m27\u001b[0m:    selected_class_zone_pathexport = exporting_to_ee(input_data = final_zone_vector, \u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exporting_to_ee' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "######### EXPORTING\n",
    "### NO NEED TO TO IT AGAIN IF THERE IS NO CHANGES\n",
    "\n",
    "from osi.exporting_ee.main import exporting_from_ee\n",
    "\n",
    "training_points_pathexport = exporting_from_ee(input_data = training_points, \n",
    "                                  type_data_export='vector', \n",
    "                                  output='shp',\n",
    "                                  name_file_desc=f'training_points_{project_name}', \n",
    "                                  folder_name=f'result_lu_class_zone_{project_name}' )\n",
    "\n",
    "validation_points_pathexport = exporting_from_ee(input_data = validation_points, \n",
    "                                  type_data_export='vector', \n",
    "                                  output='shp',\n",
    "                                  name_file_desc=f'validation_points_{project_name}', \n",
    "                                  folder_name=f'result_lu_class_zone_{project_name}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export task status: {'state': 'READY', 'description': 'zone_result_project_colombia_gbm_3_cluster', 'priority': 100, 'creation_timestamp_ms': 1725995728777, 'update_timestamp_ms': 1725995728777, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_FEATURES', 'id': 'A32GCU3ZR4EC3CPWLWMLKCHC', 'name': 'projects/ee-iwansetiawan/operations/A32GCU3ZR4EC3CPWLWMLKCHC'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'zone_result_project_colombia_gbm_3_cluster', 'priority': 100, 'creation_timestamp_ms': 1725995728777, 'update_timestamp_ms': 1725995734556, 'start_timestamp_ms': 1725995734410, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'id': 'A32GCU3ZR4EC3CPWLWMLKCHC', 'name': 'projects/ee-iwansetiawan/operations/A32GCU3ZR4EC3CPWLWMLKCHC'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'zone_result_project_colombia_gbm_3_cluster', 'priority': 100, 'creation_timestamp_ms': 1725995728777, 'update_timestamp_ms': 1725995744556, 'start_timestamp_ms': 1725995734410, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'batch_eecu_usage_seconds': 27.839767456, 'id': 'A32GCU3ZR4EC3CPWLWMLKCHC', 'name': 'projects/ee-iwansetiawan/operations/A32GCU3ZR4EC3CPWLWMLKCHC'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'zone_result_project_colombia_gbm_3_cluster', 'priority': 100, 'creation_timestamp_ms': 1725995728777, 'update_timestamp_ms': 1725995754556, 'start_timestamp_ms': 1725995734410, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'batch_eecu_usage_seconds': 43.475811004, 'id': 'A32GCU3ZR4EC3CPWLWMLKCHC', 'name': 'projects/ee-iwansetiawan/operations/A32GCU3ZR4EC3CPWLWMLKCHC'}\n",
      "Export task status: {'state': 'RUNNING', 'description': 'zone_result_project_colombia_gbm_3_cluster', 'priority': 100, 'creation_timestamp_ms': 1725995728777, 'update_timestamp_ms': 1725995771860, 'start_timestamp_ms': 1725995734410, 'task_type': 'EXPORT_FEATURES', 'attempt': 1, 'id': 'A32GCU3ZR4EC3CPWLWMLKCHC', 'name': 'projects/ee-iwansetiawan/operations/A32GCU3ZR4EC3CPWLWMLKCHC'}\n",
      "Export task completed: zone_result_project_colombia_gbm_3_cluster\n",
      "location in gdrive (please add location drive letter) --> \\My Drive\\result_lu_class_zone_project_colombia\\zone_result_project_colombia_gbm_3_cluster.shp\n"
     ]
    }
   ],
   "source": [
    "# exporting to the vectors\n",
    "final_zone_vector = final_zone.select('Class').reduceToVectors(\n",
    "    geometryType='polygon',\n",
    "    reducer=ee.Reducer.countEvery(),\n",
    "    scale=pca_scale,\n",
    "    maxPixels=1e10,\n",
    "    geometry=AOI  # Add geometry parameter\n",
    ")\n",
    "\n",
    "selected_class_zone_pathexport = exporting_from_ee(input_data = final_zone_vector, \n",
    "                                  type_data_export='vector', \n",
    "                                  output='shp',\n",
    "                                  name_file_desc=f'zone_result_{project_name}_{algo_ml_selected}_{config[\"super_pixel_size\"]}_cluster', \n",
    "                                  folder_name=f'result_lu_class_zone_{project_name}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the image to Google Drive as a GeoTIFF\n",
    "# export_task = ee.batch.Export.image.toDrive(\n",
    "#     image=object_properties_image,  # Clip the image to the ROI\n",
    "#     description=f'cluster_{project_name}_{algo_ml_selected}_{config[\"super_pixel_size\"]}m',  # Description for the export task\n",
    "#     folder=f'result_input_{project_name}',  # Optional: Folder in Google Drive to save the file\n",
    "#     fileNamePrefix=f'cluster_{project_name}_{algo_ml_selected}_{config[\"super_pixel_size\"]}m',  # File name prefix for the exported TIFF\n",
    "#     region=AOI.geometry().getInfo()['coordinates'],  # The region to export (in GeoJSON format)\n",
    "#     scale=pca_scale,  # Pixel resolution in meters (e.g., 10 for Sentinel-2)\n",
    "#     crs='EPSG:4326',  # Coordinate reference system (e.g., WGS84)\n",
    "#     maxPixels=1e13,  # Maximum number of pixels allowed for the export\n",
    "#     fileFormat='GeoTIFF'  # File format for the exported image\n",
    "# )\n",
    "\n",
    "# # Start the export task\n",
    "# export_task.start()\n",
    "\n",
    "# # Monitor the task status\n",
    "# import time\n",
    "# while export_task.active():\n",
    "#     print('Export task status:', export_task.status())\n",
    "#     time.sleep(10)\n",
    "\n",
    "# print(f'Export task completed')\n",
    "\n",
    "# cluster_path_exported = exporting_to_ee(input_data = object_properties_image, \n",
    "#                                   type_data_export='raster', \n",
    "#                                   output='GeoTIFF',\n",
    "#                                   name_file_desc=f'cluster_{project_name}_{algo_ml_selected}_{config[\"super_pixel_size\"]}m', \n",
    "#                                   folder_name=f'result_input_{project_name}',\n",
    "#                                   AOI = AOI,\n",
    "#                                   pca_scale=pca_scale )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'code_lu' values from both shapefiles: [1, 2, 3, 4, 9]\n",
      "Original labels: [1 2 3 4 9]\n",
      "Encoded labels: [0 1 2 3 4]\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 62ms/step - loss: 12.8242 - accuracy: 0.2180 - val_loss: 11.3477 - val_accuracy: 0.2985\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.4495 - accuracy: 0.4511 - val_loss: 9.3907 - val_accuracy: 0.3284\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5430 - accuracy: 0.4436 - val_loss: 4.4748 - val_accuracy: 0.4776\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.2897 - accuracy: 0.5038 - val_loss: 2.7533 - val_accuracy: 0.4478\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1046 - accuracy: 0.5113 - val_loss: 1.8291 - val_accuracy: 0.4030\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2455 - accuracy: 0.5639 - val_loss: 1.6643 - val_accuracy: 0.5075\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0464 - accuracy: 0.5940 - val_loss: 1.0389 - val_accuracy: 0.6269\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7390 - accuracy: 0.7444 - val_loss: 0.9647 - val_accuracy: 0.7463\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6657 - accuracy: 0.7068 - val_loss: 1.0953 - val_accuracy: 0.5821\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6236 - accuracy: 0.7143 - val_loss: 0.8093 - val_accuracy: 0.7463\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5410 - accuracy: 0.7444 - val_loss: 0.7965 - val_accuracy: 0.7612\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5816 - accuracy: 0.7444 - val_loss: 0.8022 - val_accuracy: 0.7463\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5421 - accuracy: 0.7218 - val_loss: 0.7426 - val_accuracy: 0.7463\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4885 - accuracy: 0.8045 - val_loss: 0.7128 - val_accuracy: 0.7463\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4718 - accuracy: 0.8271 - val_loss: 0.7565 - val_accuracy: 0.7910\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4559 - accuracy: 0.7970 - val_loss: 0.8319 - val_accuracy: 0.7015\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4792 - accuracy: 0.8346 - val_loss: 0.7373 - val_accuracy: 0.7463\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4792 - accuracy: 0.7820 - val_loss: 1.0379 - val_accuracy: 0.6716\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5581 - accuracy: 0.7218 - val_loss: 0.6869 - val_accuracy: 0.7612\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5096 - accuracy: 0.8045 - val_loss: 0.8228 - val_accuracy: 0.6866\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4997 - accuracy: 0.8045 - val_loss: 0.8650 - val_accuracy: 0.7463\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5403 - accuracy: 0.7820 - val_loss: 0.6920 - val_accuracy: 0.7313\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4491 - accuracy: 0.8271 - val_loss: 0.8438 - val_accuracy: 0.7164\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4325 - accuracy: 0.8195 - val_loss: 0.7981 - val_accuracy: 0.7761\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4715 - accuracy: 0.8120 - val_loss: 1.0474 - val_accuracy: 0.7015\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4535 - accuracy: 0.7895 - val_loss: 0.7194 - val_accuracy: 0.7761\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4503 - accuracy: 0.8195 - val_loss: 0.6877 - val_accuracy: 0.8507\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5049 - accuracy: 0.7820 - val_loss: 0.7489 - val_accuracy: 0.7910\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4051 - accuracy: 0.8647 - val_loss: 0.8157 - val_accuracy: 0.7463\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5902 - accuracy: 0.7444 - val_loss: 1.0459 - val_accuracy: 0.7164\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4422 - accuracy: 0.8120 - val_loss: 0.9002 - val_accuracy: 0.7612\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5881 - accuracy: 0.7970 - val_loss: 1.3272 - val_accuracy: 0.6418\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5241 - accuracy: 0.7669 - val_loss: 0.8316 - val_accuracy: 0.7761\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5143 - accuracy: 0.8195 - val_loss: 1.0874 - val_accuracy: 0.6866\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4445 - accuracy: 0.8346 - val_loss: 0.8043 - val_accuracy: 0.7463\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4363 - accuracy: 0.8571 - val_loss: 0.9003 - val_accuracy: 0.7313\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4166 - accuracy: 0.7820 - val_loss: 0.7341 - val_accuracy: 0.8060\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4608 - accuracy: 0.8045 - val_loss: 0.7700 - val_accuracy: 0.7761\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3445 - accuracy: 0.8872 - val_loss: 0.6735 - val_accuracy: 0.7910\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4208 - accuracy: 0.7895 - val_loss: 0.9713 - val_accuracy: 0.7015\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4486 - accuracy: 0.8120 - val_loss: 0.7090 - val_accuracy: 0.7612\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3975 - accuracy: 0.8346 - val_loss: 1.1092 - val_accuracy: 0.6866\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4296 - accuracy: 0.7669 - val_loss: 0.8927 - val_accuracy: 0.7612\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.8365 - accuracy: 0.7368 - val_loss: 1.2482 - val_accuracy: 0.5970\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4884 - accuracy: 0.8271 - val_loss: 0.7592 - val_accuracy: 0.7164\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6952 - accuracy: 0.7444 - val_loss: 1.3214 - val_accuracy: 0.6567\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7252 - accuracy: 0.7293 - val_loss: 0.8291 - val_accuracy: 0.7612\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7332 - accuracy: 0.7444 - val_loss: 1.1946 - val_accuracy: 0.6418\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5224 - accuracy: 0.8045 - val_loss: 1.0381 - val_accuracy: 0.7463\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3798 - accuracy: 0.8120 - val_loss: 0.8837 - val_accuracy: 0.7761\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4629 - accuracy: 0.8571 - val_loss: 1.0870 - val_accuracy: 0.7463\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5237 - accuracy: 0.8722 - val_loss: 1.2363 - val_accuracy: 0.7015\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5524 - accuracy: 0.8195 - val_loss: 0.9202 - val_accuracy: 0.7612\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3720 - accuracy: 0.8496 - val_loss: 0.7740 - val_accuracy: 0.8060\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3994 - accuracy: 0.8421 - val_loss: 0.8464 - val_accuracy: 0.7761\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4065 - accuracy: 0.8271 - val_loss: 0.7594 - val_accuracy: 0.7761\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3836 - accuracy: 0.8120 - val_loss: 0.8135 - val_accuracy: 0.7463\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3111 - accuracy: 0.8872 - val_loss: 0.8385 - val_accuracy: 0.7761\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3588 - accuracy: 0.8647 - val_loss: 0.8137 - val_accuracy: 0.7761\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2989 - accuracy: 0.9023 - val_loss: 0.9380 - val_accuracy: 0.7612\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3469 - accuracy: 0.8571 - val_loss: 0.8642 - val_accuracy: 0.7313\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3003 - accuracy: 0.8947 - val_loss: 0.8260 - val_accuracy: 0.7910\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3309 - accuracy: 0.8872 - val_loss: 0.7762 - val_accuracy: 0.7910\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3015 - accuracy: 0.8947 - val_loss: 0.7771 - val_accuracy: 0.7313\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3129 - accuracy: 0.9023 - val_loss: 0.8058 - val_accuracy: 0.7313\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2802 - accuracy: 0.9023 - val_loss: 0.9051 - val_accuracy: 0.7164\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2994 - accuracy: 0.8872 - val_loss: 0.7555 - val_accuracy: 0.7612\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2950 - accuracy: 0.9098 - val_loss: 1.0875 - val_accuracy: 0.7164\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3455 - accuracy: 0.8872 - val_loss: 0.7708 - val_accuracy: 0.7463\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3086 - accuracy: 0.8947 - val_loss: 0.9791 - val_accuracy: 0.7164\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3112 - accuracy: 0.8947 - val_loss: 0.8359 - val_accuracy: 0.7761\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3389 - accuracy: 0.8421 - val_loss: 1.0002 - val_accuracy: 0.7164\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4070 - accuracy: 0.8271 - val_loss: 0.7596 - val_accuracy: 0.8060\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5176 - accuracy: 0.7594 - val_loss: 1.0518 - val_accuracy: 0.7463\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3628 - accuracy: 0.8421 - val_loss: 0.7835 - val_accuracy: 0.7612\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3280 - accuracy: 0.8421 - val_loss: 0.9518 - val_accuracy: 0.7612\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3636 - accuracy: 0.8947 - val_loss: 0.8481 - val_accuracy: 0.7463\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4270 - accuracy: 0.8271 - val_loss: 0.9341 - val_accuracy: 0.7463\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4638 - accuracy: 0.8271 - val_loss: 0.9562 - val_accuracy: 0.7313\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3183 - accuracy: 0.8947 - val_loss: 0.7492 - val_accuracy: 0.7910\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3500 - accuracy: 0.8872 - val_loss: 1.0021 - val_accuracy: 0.7761\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2851 - accuracy: 0.9023 - val_loss: 0.7385 - val_accuracy: 0.7910\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2968 - accuracy: 0.8947 - val_loss: 0.8923 - val_accuracy: 0.7463\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3034 - accuracy: 0.8947 - val_loss: 0.7476 - val_accuracy: 0.8209\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2789 - accuracy: 0.9023 - val_loss: 0.8522 - val_accuracy: 0.7313\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2935 - accuracy: 0.9023 - val_loss: 0.7849 - val_accuracy: 0.7612\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2947 - accuracy: 0.8797 - val_loss: 0.8659 - val_accuracy: 0.7463\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2682 - accuracy: 0.9023 - val_loss: 0.7589 - val_accuracy: 0.7463\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3420 - accuracy: 0.8496 - val_loss: 1.0831 - val_accuracy: 0.7313\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5647 - accuracy: 0.8045 - val_loss: 0.9531 - val_accuracy: 0.7612\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4305 - accuracy: 0.8421 - val_loss: 0.8591 - val_accuracy: 0.6866\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3733 - accuracy: 0.8571 - val_loss: 0.9642 - val_accuracy: 0.7612\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2790 - accuracy: 0.9023 - val_loss: 1.0194 - val_accuracy: 0.7164\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3248 - accuracy: 0.8872 - val_loss: 0.9788 - val_accuracy: 0.7463\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2990 - accuracy: 0.8647 - val_loss: 0.7946 - val_accuracy: 0.7910\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3869 - accuracy: 0.8722 - val_loss: 1.1051 - val_accuracy: 0.6716\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5347 - accuracy: 0.7669 - val_loss: 0.8393 - val_accuracy: 0.7612\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5213 - accuracy: 0.8045 - val_loss: 1.0977 - val_accuracy: 0.7164\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3677 - accuracy: 0.8421 - val_loss: 0.7941 - val_accuracy: 0.8209\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2893 - accuracy: 0.8722 - val_loss: 1.0219 - val_accuracy: 0.7313\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0219 - accuracy: 0.7313\n",
      "Validation Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "## DEEP LEARNING start with Tensorflow!!!!!\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.mask import mask\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "arc_ops.list_layers = map.listLayers()\n",
    "arc_ops.list_source_layers_in_map = [safe_get_data_source(layer) for layer in arc_ops.list_layers]\n",
    "\n",
    "# load class label training - \n",
    "# STATIC HARD CODED, SINCE PROBABLY NEXT RUN WE DONT WANT TO RUN THE SCRIPT FROM THE BEGINNING (NO NEED TO DO EXPORTING AGAIN)\n",
    "# if you want to change just uncomment the first one os.path.join\n",
    "# path_training = os.path.join('G:',training_points_pathexport)\n",
    "path_training = 'G:\\\\My Drive\\\\result_lu_class_zone_belaban_rayak\\\\training_points_belaban_rayak.shp'\n",
    "# path_validation = os.path.join('G:',validation_points_pathexport)\n",
    "path_validation = 'G:\\\\My Drive\\\\result_lu_class_zone_belaban_rayak\\\\validation_points_belaban_rayak.shp'\n",
    "# path_training\n",
    "\n",
    "# Load the raster data (TIFF) with multiple bands\n",
    "raster_path = r\"G:\\My Drive\\result_input_belaban_rayak\\cluster_belaban_rayak_gbm_3m.tif\"\n",
    "# raster_path = os.path.join('G:',cluster_path_exported)\n",
    "\n",
    "# Function to extract pixel values for each point in the shapefile\n",
    "def extract_raster_values(raster, points):\n",
    "    values = []\n",
    "    labels = []\n",
    "    for index, point in points.iterrows():\n",
    "        # Convert point geometry to rasterio-friendly format\n",
    "        coords = [point.geometry]\n",
    "        \n",
    "        # Mask the raster using the point geometry to get pixel values\n",
    "        out_image, out_transform = mask(raster, coords, crop=True)\n",
    "        \n",
    "        # Flatten and extract pixel values\n",
    "        pixel_values = out_image[:, 0, 0]  # Extract values from the masked array\n",
    "        values.append(pixel_values)\n",
    "        \n",
    "        # Extract class label\n",
    "        labels.append(point['code_lu'])  # Replace 'class_label' with your label column name\n",
    "\n",
    "    return np.array(values), np.array(labels)\n",
    "\n",
    "def run_simpleDL_tensorflow(path_training, path_validation, raster_path, epochs=100):\n",
    "    raster = rasterio.open(raster_path)\n",
    "    training_points = gpd.read_file(path_training)\n",
    "    validation_points = gpd.read_file(path_validation)\n",
    "\n",
    "    # Get unique values from 'code_lu' column in each GeoDataFrame\n",
    "    unique_code_lu_training = training_points['code_lu'].unique()\n",
    "    unique_code_lu_validation = validation_points['code_lu'].unique()\n",
    "\n",
    "    # Combine unique values from both GeoDataFrames and get unique values again\n",
    "    combined_unique_code_lu = list(set(unique_code_lu_training) | set(unique_code_lu_validation))\n",
    "\n",
    "    label_unique_array = np.array(combined_unique_code_lu)\n",
    "\n",
    "    # Print the combined unique values of 'code_lu'\n",
    "    print(\"Unique 'code_lu' values from both shapefiles:\", combined_unique_code_lu)\n",
    "    # Display the mapping\n",
    "    print(\"Original labels:\", label_unique_array) # dealing with the 1,2,3,4,9 --> 9 give error\n",
    "\n",
    "    # Initialize LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the labels to consecutive integers (0, 1, 2, 3, 4)\n",
    "    encoded_labels = label_encoder.fit_transform(label_unique_array)\n",
    "    print(\"Encoded labels:\", encoded_labels)\n",
    "    \n",
    "    # Extract input features (raster pixel values) and output labels (class labels) for training and validation sets\n",
    "    X_train, y_train = extract_raster_values(raster, training_points)\n",
    "    X_val, y_val = extract_raster_values(raster, validation_points)\n",
    "\n",
    "    # Fit and transform the labels to consecutive integers (0, 1, 2, 3, ...)\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)  # Use the same encoder to transform validation labels\n",
    "\n",
    "    # Build a simple neural network model using TensorFlow\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer\n",
    "        tf.keras.layers.Dense(len(np.unique(y_train_encoded)), activation='softmax')  # Output layer (softmax for multi-class classification)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with encoded labels\n",
    "    history = model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=32, validation_data=(X_val, y_val_encoded))\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val_encoded)\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}')\n",
    "    \n",
    "# raster on cluster\n",
    "run_simpleDL_tensorflow(path_training, path_validation, raster_path, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Export the all image to Google Drive as a GeoTIFF - takes time to export, no need to export again\n",
    "# export_task = ee.batch.Export.image.toDrive(\n",
    "#     image=image_norm_with_spectral_indices_FCD,  # Clip the image to the ROI\n",
    "#     description=f'image_all_{project_name}_normalized',  # Description for the export task\n",
    "#     folder=f'result_input_{project_name}',  # Optional: Folder in Google Drive to save the file\n",
    "#     fileNamePrefix=f'image_all_{project_name}_normalized',  # File name prefix for the exported TIFF\n",
    "#     region=AOI.geometry().getInfo()['coordinates'],  # The region to export (in GeoJSON format)\n",
    "#     scale=pca_scale,  # Pixel resolution in meters (e.g., 10 for Sentinel-2)\n",
    "#     crs='EPSG:4326',  # Coordinate reference system (e.g., WGS84)\n",
    "#     maxPixels=1e13,  # Maximum number of pixels allowed for the export\n",
    "#     fileFormat='GeoTIFF'  # File format for the exported image\n",
    "# )\n",
    "\n",
    "# # Start the export task\n",
    "# export_task.start()\n",
    "\n",
    "# # Monitor the task status\n",
    "# import time\n",
    "# while export_task.active():\n",
    "#     print('Export task status:', export_task.status())\n",
    "#     time.sleep(10)\n",
    "\n",
    "# print(f'Export task completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'code_lu' values from both shapefiles: [1, 2, 3, 4, 9]\n",
      "Original labels: [1 2 3 4 9]\n",
      "Encoded labels: [0 1 2 3 4]\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 4.5423 - accuracy: 0.2030 - val_loss: 1.5542 - val_accuracy: 0.5373\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2217 - accuracy: 0.5038 - val_loss: 1.0939 - val_accuracy: 0.6567\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7198 - accuracy: 0.7519 - val_loss: 0.8460 - val_accuracy: 0.7463\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6892 - accuracy: 0.7293 - val_loss: 0.8118 - val_accuracy: 0.7612\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6787 - accuracy: 0.7068 - val_loss: 0.6471 - val_accuracy: 0.7463\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5411 - accuracy: 0.7970 - val_loss: 0.6021 - val_accuracy: 0.7164\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4675 - accuracy: 0.8120 - val_loss: 0.4808 - val_accuracy: 0.7612\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4612 - accuracy: 0.8045 - val_loss: 0.5904 - val_accuracy: 0.7463\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4618 - accuracy: 0.8421 - val_loss: 0.5625 - val_accuracy: 0.8060\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4371 - accuracy: 0.8120 - val_loss: 0.4578 - val_accuracy: 0.8358\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4690 - accuracy: 0.8195 - val_loss: 0.4555 - val_accuracy: 0.8209\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4157 - accuracy: 0.8496 - val_loss: 0.4776 - val_accuracy: 0.7910\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4652 - accuracy: 0.8120 - val_loss: 0.4972 - val_accuracy: 0.7910\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4072 - accuracy: 0.8421 - val_loss: 0.4516 - val_accuracy: 0.8358\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3753 - accuracy: 0.8647 - val_loss: 0.5064 - val_accuracy: 0.7910\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4033 - accuracy: 0.8346 - val_loss: 0.5687 - val_accuracy: 0.7761\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4741 - accuracy: 0.8346 - val_loss: 0.4753 - val_accuracy: 0.8060\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4942 - accuracy: 0.7744 - val_loss: 0.5172 - val_accuracy: 0.8060\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3808 - accuracy: 0.8195 - val_loss: 0.4459 - val_accuracy: 0.7463\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3680 - accuracy: 0.8647 - val_loss: 0.6163 - val_accuracy: 0.8209\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3852 - accuracy: 0.8195 - val_loss: 0.5010 - val_accuracy: 0.7612\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3995 - accuracy: 0.8571 - val_loss: 0.5272 - val_accuracy: 0.7761\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3557 - accuracy: 0.8647 - val_loss: 0.5082 - val_accuracy: 0.8060\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3571 - accuracy: 0.8571 - val_loss: 0.4871 - val_accuracy: 0.7761\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3752 - accuracy: 0.8421 - val_loss: 0.4761 - val_accuracy: 0.8060\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3302 - accuracy: 0.8872 - val_loss: 0.5094 - val_accuracy: 0.7761\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3562 - accuracy: 0.8496 - val_loss: 0.5166 - val_accuracy: 0.7761\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3498 - accuracy: 0.8872 - val_loss: 0.5615 - val_accuracy: 0.7313\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3550 - accuracy: 0.8647 - val_loss: 0.4940 - val_accuracy: 0.7612\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3397 - accuracy: 0.8947 - val_loss: 0.4867 - val_accuracy: 0.8060\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3793 - accuracy: 0.8195 - val_loss: 0.4846 - val_accuracy: 0.7910\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3442 - accuracy: 0.9023 - val_loss: 0.4959 - val_accuracy: 0.7463\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3285 - accuracy: 0.8872 - val_loss: 0.5662 - val_accuracy: 0.8209\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3709 - accuracy: 0.8421 - val_loss: 0.5396 - val_accuracy: 0.7761\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4010 - accuracy: 0.8421 - val_loss: 0.5428 - val_accuracy: 0.7910\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4535 - accuracy: 0.8195 - val_loss: 0.7035 - val_accuracy: 0.7910\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4292 - accuracy: 0.8271 - val_loss: 0.5559 - val_accuracy: 0.7910\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5036 - accuracy: 0.7970 - val_loss: 0.5718 - val_accuracy: 0.7761\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4621 - accuracy: 0.8120 - val_loss: 0.5263 - val_accuracy: 0.7463\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3554 - accuracy: 0.8797 - val_loss: 0.5145 - val_accuracy: 0.8209\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3097 - accuracy: 0.8947 - val_loss: 0.6344 - val_accuracy: 0.7761\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3486 - accuracy: 0.8872 - val_loss: 0.5183 - val_accuracy: 0.8358\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3454 - accuracy: 0.8722 - val_loss: 0.5591 - val_accuracy: 0.8060\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4025 - accuracy: 0.8271 - val_loss: 0.5704 - val_accuracy: 0.7612\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4042 - accuracy: 0.8421 - val_loss: 0.5206 - val_accuracy: 0.8060\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4234 - accuracy: 0.8421 - val_loss: 0.7126 - val_accuracy: 0.8060\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5014 - accuracy: 0.7970 - val_loss: 0.6453 - val_accuracy: 0.7164\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4879 - accuracy: 0.8346 - val_loss: 0.6341 - val_accuracy: 0.7612\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3053 - accuracy: 0.8947 - val_loss: 0.7961 - val_accuracy: 0.7910\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3815 - accuracy: 0.8571 - val_loss: 0.6445 - val_accuracy: 0.7761\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3405 - accuracy: 0.8722 - val_loss: 0.6022 - val_accuracy: 0.7761\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3552 - accuracy: 0.8571 - val_loss: 0.6863 - val_accuracy: 0.8060\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2934 - accuracy: 0.8797 - val_loss: 0.6187 - val_accuracy: 0.7463\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3750 - accuracy: 0.8647 - val_loss: 0.6459 - val_accuracy: 0.7164\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2839 - accuracy: 0.8722 - val_loss: 0.6052 - val_accuracy: 0.7910\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3637 - accuracy: 0.8346 - val_loss: 0.6337 - val_accuracy: 0.7761\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2919 - accuracy: 0.8722 - val_loss: 0.6794 - val_accuracy: 0.7612\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3296 - accuracy: 0.8722 - val_loss: 0.6462 - val_accuracy: 0.8358\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.2922 - accuracy: 0.8797 - val_loss: 0.6737 - val_accuracy: 0.7761\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2877 - accuracy: 0.8722 - val_loss: 0.5198 - val_accuracy: 0.7910\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3096 - accuracy: 0.8571 - val_loss: 0.5293 - val_accuracy: 0.8209\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2696 - accuracy: 0.9023 - val_loss: 0.6075 - val_accuracy: 0.7612\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2787 - accuracy: 0.8722 - val_loss: 0.5887 - val_accuracy: 0.7761\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3047 - accuracy: 0.8571 - val_loss: 0.5918 - val_accuracy: 0.7910\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2647 - accuracy: 0.9023 - val_loss: 0.5225 - val_accuracy: 0.8060\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2623 - accuracy: 0.8947 - val_loss: 0.6142 - val_accuracy: 0.7463\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2707 - accuracy: 0.9023 - val_loss: 0.5823 - val_accuracy: 0.7761\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3089 - accuracy: 0.8571 - val_loss: 0.5471 - val_accuracy: 0.7910\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2652 - accuracy: 0.8947 - val_loss: 0.6973 - val_accuracy: 0.7463\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3252 - accuracy: 0.8947 - val_loss: 0.6081 - val_accuracy: 0.7761\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3857 - accuracy: 0.8571 - val_loss: 0.5333 - val_accuracy: 0.7761\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3491 - accuracy: 0.8872 - val_loss: 0.7185 - val_accuracy: 0.7015\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2979 - accuracy: 0.8872 - val_loss: 0.7428 - val_accuracy: 0.7761\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3493 - accuracy: 0.8195 - val_loss: 0.6651 - val_accuracy: 0.7761\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3094 - accuracy: 0.8872 - val_loss: 0.6313 - val_accuracy: 0.8060\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2993 - accuracy: 0.8722 - val_loss: 0.6430 - val_accuracy: 0.7612\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3300 - accuracy: 0.8421 - val_loss: 0.6893 - val_accuracy: 0.7612\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2610 - accuracy: 0.9098 - val_loss: 0.6562 - val_accuracy: 0.7761\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2907 - accuracy: 0.8797 - val_loss: 0.7281 - val_accuracy: 0.7612\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4282 - accuracy: 0.8346 - val_loss: 0.7492 - val_accuracy: 0.7761\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.2866 - accuracy: 0.8647 - val_loss: 0.5947 - val_accuracy: 0.7761\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3439 - accuracy: 0.8797 - val_loss: 0.5179 - val_accuracy: 0.8060\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.2999 - accuracy: 0.8722 - val_loss: 0.6329 - val_accuracy: 0.7761\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3428 - accuracy: 0.8722 - val_loss: 0.6120 - val_accuracy: 0.7612\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3412 - accuracy: 0.8722 - val_loss: 0.5487 - val_accuracy: 0.7612\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4016 - accuracy: 0.8421 - val_loss: 0.6341 - val_accuracy: 0.7761\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2695 - accuracy: 0.9098 - val_loss: 0.6267 - val_accuracy: 0.7612\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2790 - accuracy: 0.8872 - val_loss: 0.5867 - val_accuracy: 0.7761\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2451 - accuracy: 0.8947 - val_loss: 0.5825 - val_accuracy: 0.7761\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2558 - accuracy: 0.8947 - val_loss: 0.5642 - val_accuracy: 0.7761\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2359 - accuracy: 0.9023 - val_loss: 0.5226 - val_accuracy: 0.8060\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2424 - accuracy: 0.8947 - val_loss: 0.5799 - val_accuracy: 0.7612\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2401 - accuracy: 0.9098 - val_loss: 0.6329 - val_accuracy: 0.7910\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2820 - accuracy: 0.8872 - val_loss: 0.5582 - val_accuracy: 0.8209\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2962 - accuracy: 0.8722 - val_loss: 0.6842 - val_accuracy: 0.7612\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2786 - accuracy: 0.8947 - val_loss: 0.6486 - val_accuracy: 0.7910\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2527 - accuracy: 0.9023 - val_loss: 0.5833 - val_accuracy: 0.8209\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3493 - accuracy: 0.8421 - val_loss: 0.6059 - val_accuracy: 0.7612\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4809 - accuracy: 0.8346 - val_loss: 0.8610 - val_accuracy: 0.7015\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3882 - accuracy: 0.8571 - val_loss: 0.8725 - val_accuracy: 0.7313\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8725 - accuracy: 0.7313\n",
      "Validation Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "# STATIC HARD CODED PATH, REMEMBER TO EDIT THIS\n",
    "raster_pixel = r\"G:\\My Drive\\result_input_belaban_rayak\\image_all_belaban_rayak_normalized.tif\"\n",
    "# raster on cluster\n",
    "run_simpleDL_tensorflow(path_training, path_validation, raster_pixel, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arcgis deeplearning\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from arcgis.learn import prepare_data, SingleShotDetector\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Load the training point shapefile\n",
    "path_training = 'G:\\\\My Drive\\\\result_lu_class_zone_belaban_rayak\\\\training_points_belaban_rayak.shp' \n",
    "gdf_training = gpd.read_file(path_training)\n",
    "\n",
    "# Load the raster data\n",
    "raster_path = r\"G:\\My Drive\\result_input_belaban_rayak\\cluster_belaban_rayak_gbm_3m.tif\" # cluster image path exported from gee\n",
    "raster = rasterio.open(raster_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace:  C:\\Users\\q_bal\\Documents\\ArcGIS\\scratch\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid number of channels: 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "In  \u001b[0;34m[12]\u001b[0m:\nLine \u001b[0;34m33\u001b[0m:    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid number of channels: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mchip_image.shape[\u001b[34m2\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid number of channels: 15\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create directories to store image chips and annotation files\n",
    "# Get the current environment workspace\n",
    "current_workspace = arcpy.env.workspace\n",
    "# root_dir = os.path.dirname(current_workspace)\n",
    "root_dir = r'C:\\Users\\q_bal\\Documents\\ArcGIS\\scratch'\n",
    "print('workspace: ', root_dir)\n",
    "\n",
    "output_folder = os.path.join(root_dir,'exported_training_data')\n",
    "chips_folder = os.path.join(output_folder, 'images')\n",
    "annotations_folder = os.path.join(output_folder, 'labels')\n",
    "os.makedirs(chips_folder, exist_ok=True)\n",
    "os.makedirs(annotations_folder, exist_ok=True)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Example code to handle different channel cases\n",
    "chip_image = np.moveaxis(chip, 0, -1)  # Convert to (H, W, C) format if needed\n",
    "\n",
    "# Convert to uint8 type and ensure correct range [0, 255]\n",
    "chip_image = ((chip_image - chip_image.min()) / (chip_image.max() - chip_image.min()) * 255).astype(np.uint8)\n",
    "\n",
    "# Check the number of channels\n",
    "if len(chip_image.shape) == 2:\n",
    "    # Single-channel image (grayscale)\n",
    "    print(\"Converting single-channel image to 3-channel BGR...\")\n",
    "    chip_image = cv2.cvtColor(chip_image, cv2.COLOR_GRAY2BGR)\n",
    "elif chip_image.shape[2] == 1:\n",
    "    # Convert single-channel image to 3-channel BGR\n",
    "    print(\"Converting single-channel image to 3-channel BGR...\")\n",
    "    chip_image = cv2.cvtColor(chip_image, cv2.COLOR_GRAY2BGR)\n",
    "elif chip_image.shape[2] not in [3, 4]:\n",
    "    raise ValueError(f\"Invalid number of channels: {chip_image.shape[2]}\")\n",
    "\n",
    "# Save the image using cv2.imwrite\n",
    "cv2.imwrite(chip_path, chip_image)\n",
    "\n",
    "if np.isnan(chip_image).any() or np.isinf(chip_image).any():\n",
    "    raise ValueError(\"Image contains NaN or infinite values.\")\n",
    "\n",
    "# Function to extract pixel values and save image chips around each point\n",
    "def extract_chip(raster, point, buffer_size=128, chip_size=256):\n",
    "    coords = [point.geometry.buffer(buffer_size).envelope]  # Create a square buffer around the point\n",
    "    out_image, out_transform = mask(raster, coords, crop=True)  # Mask the raster to the buffered point\n",
    "    if out_image.shape[1] < chip_size or out_image.shape[2] < chip_size:\n",
    "        return None  # Skip chips that are smaller than desired size\n",
    "    # Extract the center part to get fixed-size chips\n",
    "    center_x, center_y = out_image.shape[1] // 2, out_image.shape[2] // 2\n",
    "    chip = out_image[:, center_x-chip_size//2:center_x+chip_size//2, center_y-chip_size//2:center_y+chip_size//2]\n",
    "    return chip\n",
    "\n",
    "# Iterate over each point in the shapefile and create image chips\n",
    "for idx, point in gdf_training.iterrows():\n",
    "    chip = extract_chip(raster, point)\n",
    "    if chip is not None:\n",
    "        # Save the chip as an image\n",
    "        chip_path = os.path.join(chips_folder, f'chip_{idx}.png')\n",
    "        # Transpose the image to (H, W, C) and normalize\n",
    "        chip_image = np.moveaxis(chip, 0, -1)\n",
    "        chip_image = ((chip_image - chip_image.min()) / (chip_image.max() - chip_image.min()) * 255).astype(np.uint8)\n",
    "        cv2.imwrite(chip_path, chip_image)\n",
    "\n",
    "        # Save the annotation file (e.g., in PASCAL VOC format)\n",
    "        annotation_path = os.path.join(annotations_folder, f'chip_{idx}.xml')  # Placeholder example\n",
    "        with open(annotation_path, 'w') as f:\n",
    "            f.write(f'<annotation><filename>chip_{idx}.png</filename></annotation>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\TreeO_WORKS\\\\GIS_data\\\\ArcGIS_Pro\\\\TREEO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(current_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/59da9d50ab3f36c576d2d606a18a2bb5-ae0b70256353dbff90d9cff22433e4dc/tiles/{z}/{x}/{y}\n",
      "removing and re-adding the layer name: Final_zone_ML_gbm_Hansen\n",
      "layer re-added: Final_zone_ML_gbm_Hansen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<arcpy._mp.Layer object at 0x0000015751800BD0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from osi.spectral_indices.utils import assigning_band\n",
    "from osi.classifying.utils import add_classes, add_images, select_band_if_exists, unmasked_helper\n",
    "\n",
    "forest_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(1))\n",
    "shrub_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(2))\n",
    "grass_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(3))\n",
    "openland_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(4))\n",
    "water_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(5))\n",
    "plantation_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(6))\n",
    "infrastructure_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(7))\n",
    "oil_palm_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(8))\n",
    "cropland_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(9))\n",
    "paddy_irigated_masked = selected_image_lc.updateMask(selected_image_lc.select(['classification']).eq(14))# arc_ops.adding_ee_to_arcgisPro(infrastructure_masked.randomVisualizer(),{},'test')\n",
    "\n",
    "def unmasked_helper(image, AOI_img, AOI):\n",
    "    mask_image = image.mask()\n",
    "    mask_image_inverted = mask_image.Not()\n",
    "    unmasked_image = AOI_img.unmask().updateMask(mask_image_inverted).clip(AOI)\n",
    "    return unmasked_image\n",
    "\n",
    "#sub-forest considering the fcd threshold of high density forest, regardless the ML method\n",
    "# HighForestDense = FCD.mask(FCD.gte(high_forest).And(unmaskedWaterAOI))\n",
    "unmaskedHighForest = AOI_img.unmask().updateMask(HighForestDense.mask().Not()).clip(AOI)\n",
    "yrf_forest = forest_masked.And(unmaskedHighForest)\n",
    "high_forest_fix = HighForestDense.And(forest_masked)\n",
    "\n",
    "band_name_image = config['band_name_image']\n",
    "\n",
    "######### general go-zone\n",
    "## Starting to retrieve from 10 TCL unmask\n",
    "unmaskedLoss = unmasked_helper(minLoss, AOI_img, AOI)\n",
    "# Unmasked Forest - the result is all the area outside of forest_masked (Total all forest), and now in no forest\n",
    "# maskHiF = forest_masked.mask()\n",
    "# maskHiF_inverted = maskHiF.Not()\n",
    "# unmaskedHiF = AOI_img.unmask().updateMask(maskHiF_inverted).clip(AOI)\n",
    "unmaskedHiF = unmasked_helper(forest_masked, AOI_img, AOI)\n",
    "# unmasked water\n",
    "waterinAOI = water_masked\n",
    "unmaskedWaterAOI = unmasked_helper(water_masked, AOI_img, AOI)\n",
    "# unmasked grey infrastructure\n",
    "unmasked_infrastructure = unmasked_helper(infrastructure_masked, AOI_img, AOI)\n",
    "# unmasked oil palm\n",
    "unmasked_oilpalm = unmasked_helper(oil_palm_masked, AOI_img, AOI)\n",
    "\n",
    "goZone = unmaskedLoss.And(unmaskedHiF).And(unmaskedWaterAOI).And(unmasked_infrastructure).And(unmasked_oilpalm)\n",
    "goZone_edited = ee.Image(assigning_band(config['band_name_image'],999,goZone))\n",
    "\n",
    "# forest category and no 10 years rule\n",
    "highBaselineF = forest_masked.And(unmaskedLoss)\n",
    "highf_edited = ee.Image(assigning_band(config['band_name_image'],111,highBaselineF))\n",
    "\n",
    "####### Get the overlay information of HighBaseline and Tree Cover Loss (Hansen), e.g., Young Regenerating Forest\n",
    "HiForestAndLoss = AOI_img.And(forest_masked.And(minLoss)) #minLoss is the actual TCL without overlaying with Sentinel\n",
    "tenyrfl_edited = ee.Image(assigning_band(band_name_image,888,HiForestAndLoss))\n",
    "# arc_ops.adding_ee_to_arcgisPro(tenyrfl_edited.randomVisualizer(),{},'tenyrlfl_edited')\n",
    "\n",
    "# Create a helper mask indicating where the smaller areas maskhiFL, distinguish only the highbaseline and TCL (mask) and assign mask as 1\n",
    "# Unmask the bigger raster in the areas to get the pixel value for area 'outside' HiFL (High Baseline and Forest Loss)\n",
    "unmaskedHiFL = unmasked_helper(HiForestAndLoss, AOI_img, AOI)\n",
    "#outcome result for 10 years data only that is not overlay with high baseline (Forest) unmaskedHiFL is Area that is not Forest Sentinel############\n",
    "tenYearsRule = unmaskedHiFL.And(minLoss)\n",
    "tenyrule_edited = ee.Image(assigning_band(band_name_image,8,tenYearsRule))\n",
    "# arc_ops.adding_ee_to_arcgisPro(tenyrule_edited.randomVisualizer(),{},'tenyrule_edited')\n",
    "\n",
    "#re-acquired the same method for 10 years rule data (GFW or Hansen data (pixel 30m))\n",
    "#assigning to zones:\n",
    "## GO ZONE\n",
    "Shrubland_gozone = shrub_masked.And(goZone_edited).select(['pixel'])\n",
    "Shrubland_gozone = ee.Image(assigning_band(config['band_name_image'],1,Shrubland_gozone))\n",
    "\n",
    "Grassland_gozone = grass_masked.And(goZone_edited).select(['pixel'])\n",
    "Grassland_gozone = ee.Image(assigning_band(config['band_name_image'],2,Grassland_gozone))\n",
    "\n",
    "Openland_gozone = openland_masked.And(goZone_edited).select(['pixel'])\n",
    "Openland_gozone = ee.Image(assigning_band(config['band_name_image'],3,Openland_gozone))\n",
    "\n",
    "Openland_gozone = openland_masked.And(goZone_edited).select(['pixel'])\n",
    "Openland_gozone = ee.Image(assigning_band(config['band_name_image'],3,Openland_gozone))\n",
    "\n",
    "Cropland_Gozone = cropland_masked.And(goZone_edited).select(['pixel'])\n",
    "Cropland_Gozone = ee.Image(assigning_band(config['band_name_image'],13,Cropland_Gozone))\n",
    "\n",
    "Paddy_Gozone = paddy_irigated_masked.And(goZone_edited).select(['pixel'])\n",
    "Paddy_Gozone = ee.Image(assigning_band(config['band_name_image'],14,Paddy_Gozone))\n",
    "\n",
    "## NO GO additional\n",
    "Plantation_noGozone = plantation_masked.And(goZone_edited).select(['pixel'])\n",
    "Plantation_noGozone = ee.Image(assigning_band(config['band_name_image'],10,Plantation_noGozone))\n",
    "\n",
    "Infrastructure_noGozone = infrastructure_masked.And(goZone_edited).select(['pixel'])\n",
    "Infrastructure_noGozone = ee.Image(assigning_band(config['band_name_image'],11,Infrastructure_noGozone))\n",
    "\n",
    "Oilpalm_noGozone = oil_palm_masked.And(goZone_edited).select(['pixel'])\n",
    "Oilpalm_noGozone = ee.Image(assigning_band(config['band_name_image'],12,Oilpalm_noGozone))\n",
    "\n",
    "# highbaseline\n",
    "# HighForestDense = FCD.mask(FCD.gte(high_forest).And(unmaskedWaterAOI))\n",
    "HighForestDense_no10yrs = HighForestDense.And(highf_edited).select(['pixel'])\n",
    "HighForestDense_no10yrs = ee.Image(assigning_band(config['band_name_image'],4,HighForestDense_no10yrs))\n",
    "\n",
    "#YRFForestDense = FCD.mask(FCD.gte(yrf_forest).And(unmaskedWaterAOI).And(FCD.lt(high_forest)))\n",
    "YRFForestDense_no10yrs = yrf_forest.And(highf_edited).select(['pixel'])\n",
    "YRFForestDense_no10yrs = ee.Image(assigning_band(config['band_name_image'],5,YRFForestDense_no10yrs))\n",
    "##########################\n",
    "\n",
    "# high baseline regrowth\n",
    "HighForestDense_10yrs = HighForestDense.And(tenyrfl_edited).select(['pixel'])\n",
    "HighForestDense_10yrs = ee.Image(assigning_band(config['band_name_image'],6,HighForestDense_10yrs))\n",
    "\n",
    "YRFForestDense_10yrs = yrf_forest.And(tenyrfl_edited).select(['pixel'])\n",
    "YRFForestDense_10yrs = ee.Image(assigning_band(config['band_name_image'],7,YRFForestDense_10yrs))\n",
    "############################\n",
    "\n",
    "# 10 years rule, straight forward\n",
    "# tenyrule_edited = ee.Image(assigning_band(band_name_image,8,tenYearsRule)) #tenyears rule not pass - 8  # already hard-coded set in the previous function\n",
    "No_pass_historical_years_rule = tenyrule_edited.select([config['band_name_image']])\n",
    "\n",
    "# Filter the images based on the 'Class' band, and add them one by one on map canvas!!!!!!!!!!!!!!!!!!!! # just comment this Map.addLayer if you want not to display one by one\n",
    "openland_gozone = Openland_gozone.select([config['band_name_image']])\n",
    "# Map.addLayer(openland_gozone,{'palette': ['#F89696']},'Open land - Go Zone')\n",
    "\n",
    "grassland_gozone = Grassland_gozone.select([config['band_name_image']])\n",
    "# Map.addLayer(grassland_gozone,{'palette': ['#ffff33']},'Grass land - Go Zone')\n",
    "\n",
    "shrubland_gozone = Shrubland_gozone.select([config['band_name_image']])\n",
    "# Map.addLayer(shrubland_gozone,{'palette': ['#ffe3b3']},'Shrub land - Go Zone')\n",
    "\n",
    "yrf_forest_dense_no10yrs = YRFForestDense_no10yrs.select([config['band_name_image']])\n",
    "# Map.addLayer(yrf_forest_dense_no10yrs,{'palette': ['#83ff5a']},'Low-Med Forest')\n",
    "\n",
    "high_forest_dense_no10yrs = HighForestDense_no10yrs.select(config['band_name_image'])\n",
    "# Map.addLayer(high_forest_dense_no10yrs,{'palette': ['#09ab0c']},'High- Density Forest')\n",
    "\n",
    "yrf_forest_dense_10yrs = YRFForestDense_10yrs.select([config['band_name_image']])\n",
    "# Map.addLayer(yrf_forest_dense_10yrs,{'palette': ['#ff0abe']},'Re-growth Low-Med Density Forest')\n",
    "\n",
    "high_forest_dense_10yrs = HighForestDense_10yrs.select([config['band_name_image']])\n",
    "# Map.addLayer(high_forest_dense_10yrs,{'palette': ['#ff0004']},'Re-growth High Density Forest')\n",
    "\n",
    "No_pass_historical_years_rule = tenyrule_edited.select([config['band_name_image']])\n",
    "# Map.addLayer(No_pass_historical_years_rule,{'palette': ['#ff8a1d']},'Historical deforestation no regrowth (3 or 10 years)')\n",
    "\n",
    "# since there are case water from mining (overlap 10 years rule with water), we should unmasked first\n",
    "unmasked10years = AOI_img.unmask().updateMask(No_pass_historical_years_rule.mask().Not()).clip(AOI)\n",
    "water_no_10yr = waterinAOI.And(unmasked10years).And(unmasked10years)\n",
    "Water_Un_plantable = ee.Image(assigning_band(config['band_name_image'],9,water_no_10yr)).select(config['band_name_image'])\n",
    "\n",
    "# Map.addLayer(Water_Un_plantable,{'palette': ['#1900ff']},'Water body (unplantable)')\n",
    "\n",
    "# Example band name to check\n",
    "band_name = config['band_name_image']\n",
    "\n",
    "# Select images if the band exists\n",
    "plantation_noGozone = select_band_if_exists(Plantation_noGozone, band_name)\n",
    "infrastructure_noGozone = select_band_if_exists(Infrastructure_noGozone, band_name)\n",
    "oilpalm_noGozone = select_band_if_exists(Oilpalm_noGozone, band_name)\n",
    "cropland_Gozone = select_band_if_exists(Cropland_Gozone, band_name)\n",
    "paddy_Gozone = select_band_if_exists(Paddy_Gozone, band_name)\n",
    "\n",
    "empty_image = ee.Image.constant(0).rename(config['band_name_image'])\n",
    "\n",
    "image_list_result = [\n",
    "    openland_gozone,\n",
    "    grassland_gozone,\n",
    "    shrubland_gozone,\n",
    "    high_forest_dense_no10yrs,\n",
    "    yrf_forest_dense_no10yrs,\n",
    "    high_forest_dense_10yrs,\n",
    "    yrf_forest_dense_10yrs,\n",
    "    No_pass_historical_years_rule,\n",
    "    Water_Un_plantable,\n",
    "    plantation_noGozone,\n",
    "    infrastructure_noGozone,\n",
    "    oilpalm_noGozone,\n",
    "    cropland_Gozone,\n",
    "    paddy_Gozone\n",
    "\n",
    "]\n",
    "\n",
    "# Remove None entries from the list\n",
    "image_list_result = [img for img in image_list_result if img is not None]\n",
    "\n",
    "# Create an ImageCollection from the list of images\n",
    "image_collection_result = ee.ImageCollection(image_list_result)\n",
    "\n",
    "# Apply the add_classes function to each image in the collection while merging them into the 'empty_image'\n",
    "result_collection_with_class = image_collection_result.map(lambda image: add_classes(image, empty_image))\n",
    "\n",
    "# Merge all the images in the result_collection using ee.ImageCollection.iterate()\n",
    "merged_image_result = ee.Image(result_collection_with_class.iterate(add_images, empty_image))\n",
    "\n",
    "# Cast the merged image to Int32 and set the original Class band name\n",
    "merged_image_result = merged_image_result.toInt32().rename('Class')\n",
    "merged_image_result = merged_image_result.clip(AOI)\n",
    "final_zone_map = merged_image_result\n",
    "\n",
    "arc_ops.adding_ee_to_arcgisPro(final_zone_map, vis_params_fcd_classified, f'Final_zone_ML_{algo_ml_selected}_Hansen')  # the naming probably will need to change, for some concistencies only so that you understand again later to read the codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class', 'pixel']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n",
      "['Class']\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in image_list_result:\n",
    "    print(i.bandNames().getInfo())\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shrubland_gozone = shrub_masked.And(goZone_edited).select(['pixel'])\n",
    "Shrubland_gozone = ee.Image(assigning_band(config['band_name_image'],1,Shrubland_gozone))\n",
    "arc_ops.adding_ee_to_arcgisPro(Shrubland_gozone.randomVisualizer(),{},'Shrubland_gozone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://earthengine.googleapis.com/v1/projects/ee-iwansetiawan/maps/cfd6d00f3129b288819a3a87e754e3fc-cc3e835c648791ff451016605e6bb581/tiles/{z}/{x}/{y}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<arcpy._mp.Layer object at 0x0000016E53FBA850>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paddy_Gozone = paddy_irigated_masked.And(goZone_edited).select(['pixel'])\n",
    "Paddy_Gozone = ee.Image(assigning_band(config['band_name_image'],14,Paddy_Gozone))\n",
    "arc_ops.adding_ee_to_arcgisPro(Paddy_Gozone.randomVisualizer(),{},'Paddy_Gozone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FCD']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HighForestDense.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Class', 'pixel']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goZone_edited.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Image.select: Band pattern 'pixel' did not match any bands. Available bands: [classification]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "In  \u001b[0;34m[59]\u001b[0m:\nLine \u001b[0;34m12\u001b[0m:    arc_ops.adding_ee_to_arcgisPro(final_zone, vis_params_fcd_classified, \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mFinal_zone_ML_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00malgo_ml_selected\u001b[33m}\u001b[39;49;00m\u001b[33m_Hansen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)  \u001b[37m# the naming probably will need to change, for some concistencies only so that you understand again later to read the codes\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\q_bal\\Documents\\Github\\GEE_notebook_Forestry\\osi\\arcpy\\main.py\u001b[0m, in \u001b[0;32madding_ee_to_arcgisPro\u001b[0m:\nLine \u001b[0;34m56\u001b[0m:    map_id_dict = ee_image.getMapId(vis_params)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\q_bal\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-cloned\\Lib\\site-packages\\ee\\image.py\u001b[0m, in \u001b[0;32mgetMapId\u001b[0m:\nLine \u001b[0;34m196\u001b[0m:   response = data.getMapId(request)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\q_bal\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-cloned\\Lib\\site-packages\\ee\\data.py\u001b[0m, in \u001b[0;32mgetMapId\u001b[0m:\nLine \u001b[0;34m745\u001b[0m:   result = _execute_cloud_call(\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\q_bal\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-cloned\\Lib\\site-packages\\ee\\data.py\u001b[0m, in \u001b[0;32m_execute_cloud_call\u001b[0m:\nLine \u001b[0;34m405\u001b[0m:   \u001b[34mraise\u001b[39;49;00m _translate_cloud_exception(e)  \u001b[37m# pylint: disable=raise-missing-from\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mEEException\u001b[0m: Image.select: Band pattern 'pixel' did not match any bands. Available bands: [classification]\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-arcgispro-py3-cloned] *",
   "language": "python",
   "name": "conda-env-conda-arcgispro-py3-cloned-py"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
